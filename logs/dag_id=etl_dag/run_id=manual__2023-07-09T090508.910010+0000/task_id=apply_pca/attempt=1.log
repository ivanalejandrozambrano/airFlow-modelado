[2023-07-09T09:05:16.920+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_dag.apply_pca manual__2023-07-09T09:05:08.910010+00:00 [queued]>
[2023-07-09T09:05:16.934+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_dag.apply_pca manual__2023-07-09T09:05:08.910010+00:00 [queued]>
[2023-07-09T09:05:16.935+0000] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-09T09:05:16.962+0000] {taskinstance.py:1327} INFO - Executing <Task(PythonOperator): apply_pca> on 2023-07-09 09:05:08.910010+00:00
[2023-07-09T09:05:16.973+0000] {standard_task_runner.py:57} INFO - Started process 3445 to run task
[2023-07-09T09:05:16.977+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'etl_dag', 'apply_pca', 'manual__2023-07-09T09:05:08.910010+00:00', '--job-id', '138', '--raw', '--subdir', 'DAGS_FOLDER/test.py', '--cfg-path', '/tmp/tmpo7aohfdi']
[2023-07-09T09:05:16.979+0000] {standard_task_runner.py:85} INFO - Job 138: Subtask apply_pca
[2023-07-09T09:05:17.078+0000] {task_command.py:410} INFO - Running <TaskInstance: etl_dag.apply_pca manual__2023-07-09T09:05:08.910010+00:00 [running]> on host 5865fe7b5e5b
[2023-07-09T09:05:17.327+0000] {taskinstance.py:1547} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='etl_dag' AIRFLOW_CTX_TASK_ID='apply_pca' AIRFLOW_CTX_EXECUTION_DATE='2023-07-09T09:05:08.910010+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-07-09T09:05:08.910010+00:00'
[2023-07-09T09:05:17.353+0000] {warnings.py:110} WARNING - /opt/***/dags/test.py:70: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.
  df.fillna(df.mean(), inplace=True)

[2023-07-09T09:05:17.359+0000] {warnings.py:110} WARNING - /opt/***/dags/test.py:73: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.
  df.fillna(df.median(), inplace=True)

[2023-07-09T09:05:17.363+0000] {python.py:183} INFO - Done. Returned value was:    LocationAbbr          LocationDesc  ...       PCA1       PCA2
0            AR              Arkansas  ...   2.329192  -2.050249
1            CO              Colorado  ... -11.989688  -4.003571
2            DC  District of Columbia  ...  14.517821   7.995440
3            GA               Georgia  ...  10.997803  -0.242377
4            MI              Michigan  ...   5.155159  10.091829
5            MT               Montana  ... -18.365173   2.144211
6            OR                Oregon  ... -27.124871  -4.538535
7            PR           Puerto Rico  ...  32.533893  -6.581275
8            PR           Puerto Rico  ...  32.533893  -6.581275
9            WI             Wisconsin  ...   0.612669   7.753479
10           WI             Wisconsin  ...   0.612669   7.753479
11           AL               Alabama  ...   8.257815  -1.474515
12           ID                 Idaho  ... -21.480617  -2.968670
13           ID                 Idaho  ... -21.480617  -2.968670
14           IL              Illinois  ...   2.963120   4.525560
15           KS                Kansas  ...  -4.552090  -1.200352
16           KS                Kansas  ...  -4.552090  -1.200352
17           KS                Kansas  ...  -4.552090  -1.200352
18           LA             Louisiana  ...   3.583206  -5.253806

[19 rows x 24 columns]
[2023-07-09T09:05:17.410+0000] {xcom.py:640} ERROR - ("Could not convert 'Number' with type str: tried to convert to int64", 'Conversion failed for column DataValueUnit with type object'). If you are using pickle instead of JSON for XCom, then you need to enable pickle support for XCom in your *** config or make sure to decorate your object with attr.
[2023-07-09T09:05:17.412+0000] {taskinstance.py:1824} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 73, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 2361, in xcom_push
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 73, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/xcom.py", line 243, in set
    map_index=map_index,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/xcom.py", line 632, in serialize_value
    return json.dumps(value, cls=XComEncoder).encode("UTF-8")
  File "/usr/local/lib/python3.7/json/__init__.py", line 238, in dumps
    **kw).encode(obj)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/json.py", line 104, in encode
    return super().encode(o)
  File "/usr/local/lib/python3.7/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/usr/local/lib/python3.7/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/json.py", line 91, in default
    return serialize(o)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/serialization/serde.py", line 142, in serialize
    data, classname, version, is_serialized = _serializers[qn].serialize(o)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/serialization/serializers/pandas.py", line 49, in serialize
    table = pa.Table.from_pandas(o)
  File "pyarrow/table.pxi", line 3557, in pyarrow.lib.Table.from_pandas
  File "/home/airflow/.local/lib/python3.7/site-packages/pyarrow/pandas_compat.py", line 612, in dataframe_to_arrays
    for c, f in zip(columns_to_convert, convert_fields)]
  File "/home/airflow/.local/lib/python3.7/site-packages/pyarrow/pandas_compat.py", line 612, in <listcomp>
    for c, f in zip(columns_to_convert, convert_fields)]
  File "/home/airflow/.local/lib/python3.7/site-packages/pyarrow/pandas_compat.py", line 598, in convert_column
    raise e
  File "/home/airflow/.local/lib/python3.7/site-packages/pyarrow/pandas_compat.py", line 592, in convert_column
    result = pa.array(col, type=type_, from_pandas=True, safe=safe)
  File "pyarrow/array.pxi", line 316, in pyarrow.lib.array
  File "pyarrow/array.pxi", line 83, in pyarrow.lib._ndarray_to_array
  File "pyarrow/error.pxi", line 100, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: ("Could not convert 'Number' with type str: tried to convert to int64", 'Conversion failed for column DataValueUnit with type object')
[2023-07-09T09:05:17.423+0000] {taskinstance.py:1350} INFO - Marking task as FAILED. dag_id=etl_dag, task_id=apply_pca, execution_date=20230709T090508, start_date=20230709T090516, end_date=20230709T090517
[2023-07-09T09:05:17.439+0000] {standard_task_runner.py:109} ERROR - Failed to execute job 138 for task apply_pca (("Could not convert 'Number' with type str: tried to convert to int64", 'Conversion failed for column DataValueUnit with type object'); 3445)
[2023-07-09T09:05:17.481+0000] {local_task_job_runner.py:225} INFO - Task exited with return code 1
[2023-07-09T09:05:17.515+0000] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
